{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Transformer Encoder\n",
    "\n",
    "In this notebook, you'll look deeper at the encoder mechanics, positional encoding, and how self-attention works.\n",
    "\n",
    "**[2.1 Overview](#2.1-Overview)<br>**\n",
    "**[2.2 Embedding](#2.2-Embedding)<br>**\n",
    "**[2.3 Positional Encoding](#2.3-Positional-Encoding)<br>**\n",
    "**[2.4 Self-Attention](#2.4-Self-Attention)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.1 Self-Attention Matrix Calculation](#2.4.1-Self-Attention-Matrix-Calculation)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.2 Visualization of Attention](#2.4.2-Visualization-of-Attention)<br>\n",
    "**[2.5 Multi-Head Attention](#2.5-Multi-Head-Attention)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Overview\n",
    "In the Transformer paper, both encoder and decoder are composed of $\\mathbf{N = 6}$ identical layers each, for a total of 12 layers. Each of the six encoder layers has two sub-layers: The first is a multi-head self-attention mechanism; the second is a simple, position-wise fully connected feed-forward network.  \n",
    "\n",
    "The encoder‚Äôs purpose is to encode a source sentence into hidden state vectors; the decoder uses the last representation of the state vectors to predict characters in the target language. \n",
    "\n",
    "Let's see what is happening inside an encode block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/encoder1.png\" width=\"500\"></center>\n",
    "\n",
    "<center> Figure 4. Encoder Block illustration. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder code is provided below. A number of `TransformerEncoderLayers` (default 6) are added to `self.layers`, which will be called when we forward pass through the model.\n",
    "\n",
    "When `TransformerEncoder.forward()` is called, the input source tokens (`src_tokens`) are embedded (using `embed_tokens`) and then added to the positional encodings (see the description later). After dropout and some tensor manipulation, our embedded tokens `x` are passed through each of the six `TranformerEncoderLayers`, in the loop that starts `for layer in self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import seaborn as sns\n",
    "\n",
    "from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder\n",
    "from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, args, embed_tokens, left_pad=True):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.fuse_dropout_add = args.fuse_dropout_add\n",
    "        self.fuse_relu_dropout = args.fuse_relu_dropout\n",
    "\n",
    "        embed_dim = embed_tokens.embedding_dim\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = args.max_source_positions\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            args.max_source_positions, embed_dim, self.padding_idx,\n",
    "            left_pad=left_pad,\n",
    "            learned=args.encoder_learned_pos,\n",
    "        ) if not args.no_token_positional_embeddings else None\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            TransformerEncoderLayer(args)\n",
    "            for i in range(args.encoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.normalize = args.encoder_normalize_before\n",
    "        if self.normalize:\n",
    "            self.layer_norm = FusedLayerNorm(embed_dim) if args.fuse_layer_norm else nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * self.embed_tokens(src_tokens)\n",
    "        if self.embed_positions is not None:\n",
    "            x += self.embed_positions(src_tokens)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        # The tensor needs to copy transposed because\n",
    "        # fused dropout is not capable of handing strided data\n",
    "        if self.fuse_dropout_add :\n",
    "            x = x.transpose(0, 1).contiguous()\n",
    "        else :\n",
    "            x = x.transpose(0, 1)\n",
    "\n",
    "        # compute padding mask\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
    "        if not encoder_padding_mask.any():\n",
    "            _encoder_padding_mask = None\n",
    "        else:\n",
    "            _encoder_padding_mask = encoder_padding_mask\n",
    "\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, _encoder_padding_mask)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x, encoder_padding_mask # x.shape == T x B x C, encoder_padding_mask.shape == B x T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `TransformerEncoderLayer` is a copy of the TransformerEncoderLayer class defined [here](https://github.com/NVIDIA/DeepLearningExamples/blob/8c3514071275b2805b29372f6dabe515d431416f/PyTorch/Translation/Transformer/fairseq/models/transformer.py#L420). The full implementation includes optional layer normalization (removed here for the sake of simplicity).\n",
    "\n",
    "Looking at the `forward` method, the embedded tokens `x` are passed through the self attention mechanism (explained below). By default `self.fuse_dropout_add` is `True` and `self.fuse_relu_dropout` is `False`, so the results of the self attention pass through a linear layer `fc1`, then have dropout applied, then through the second linear layer `fc2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how an encoder block works, we need the concept of *embedding*. We begin by turning each input word into a vector using an embedding algorithm. Word embedding is really all about improving the ability of networks to learn from text data. In simple terms, word embeddings are vector representations of a particular word. For more detailed information on embedding algorithms please see [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fastText](https://fasttext.cc/).\n",
    "\n",
    "Note that the embedding only happens in the bottom-most encoder. In Figure 4, the two input words are represented vectors. These are the embedding vectors. The Transformer model uses $\\mathbf{d_{model} = 512}$, however this lab, and the NVIDIA implementation, use the ‚Äútransformer-big‚Äù model $\\mathbf{d_{model} = 1024}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to tokenize some input text. Tokenization converts a sentence into a list of numbers, ending in a 2, the representation for \"end of sentence (EOS)\". The tokenized representation will be combined with the positional encoder, to create the embedded vector, which is the input to the self-attention layer shown in Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder_demos.tokenize as tok\n",
    "\n",
    "input_text = \"I am looking for a place to eat.\"\n",
    "tokens = tok.demo(input_text)\n",
    "print(\"\\nInput text:        %s\\nTokenized output: \" % input_text, tokens[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the input text and see how the tokenized output vector is altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate an input sentence in English to German\n",
    "\n",
    "import encoder_demos.functional_translation as ft\n",
    "input_sentence = \"I am looking for a place to eat.\"\n",
    "\n",
    "e, g, h = ft.demo(input_sentence)\n",
    "print(\"En:\", e)\n",
    "print(\"German:\", g)\n",
    "print('')\n",
    "print(\"H:\", h)\n",
    "print('H is the hypothesis along with an average log-likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis scores are output as log-probabilities, thus are negative. We can calculate the probability of this hypothesis as exp(H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability of H = {}'.format(round(np.exp(h),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Positional Encoding \n",
    "\n",
    "Language models need to make use of the sequential nature of words in a sentence. Since the Transformer model contains no recurrent or convolutional units, positional encodings (PEs) are used to account for the order of the words in the input sequence. The positional encodings have the same dimension, d<sub>model</sub>, as the embeddings, so that the two can be summed (see Figure 4). This allows the model to understand the position of each word in the input text.\n",
    "\n",
    "In the paper, the authors use sine and cosine functions of different frequencies for positional encoding:\n",
    "\n",
    "\n",
    "<img src=\"images/pe.png\" width=\"400\">\n",
    "where <i>pos</i> is the position and <i>i</i> is the dimension with range [0, d<sub>model</sub>/2). Let's explain the formula above via an example:\n",
    "\n",
    "Let's assume that  d<sub>model</sub> = 4. This means that word ùë§ at input sequence position <i>pos</i> ‚àà [0, ùêø‚àí1] is represented with a 4-dimensional embedding ùëí<sub>ùë§</sub> vector. Setting <i>i</i> ‚àà [0, 2), then, for even indices  of 4-dimensional embedding vector, we will use sin(pos/10000<i><sup>2i/d<sub>model</sub></sup></i>) function, whereas for odd indices, we will use cos(pos/10000<i><sup>2i/d<sub>model</sub></sup></i>). \n",
    "\n",
    "Let's call our embedding vector index <i>k</i>, where <i>k</i> ‚àà [0, 2<i>i</i>). The first position in our input sentence is pos = 0, and first index of the embedding vector is <i>k</i>=0. Now, the first PE (positional encoding) for the first dimension, <i>k</i> = 0, of the embedding vector will be sin(0/10000<sup>0/4</sup>), and the second PE for the second dimension, k = 1, will be cos(0/10000<sup>0/4</sup>). For the third, k = 2, and fourth dimensions, k =3, the PEs will be sin(0/10000<sup>2/4</sup>) and cos(0/10000<sup>2/4</sup>), respectively. \n",
    "\n",
    "Now we can write down the positional encoding for the first word of the input sequence:\n",
    "\n",
    "PE (pos =0) = [sin(0/10000<sup>0/4</sup>), cos(0/10000<sup>0/4</sup>), sin(0/10000<sup>2/4</sup>), cos(0/10000<sup>2/4</sup>)].\n",
    "\n",
    "in simple form:\n",
    "\n",
    "PE (pos =0) = [sin(0/10000<sup>0</sup>), cos(0/10000<sup>0</sup>), sin(0/100), cos(0/100)] = [0, 1, 0, 1].\n",
    "\n",
    "The next step is to add this vector to the embedding vector, e<sub>ùë§</sub>, and obtain a new vector, e'<sub>ùë§</sub>:\n",
    "\n",
    "e'<sub>ùë§</sub> = PE (pos =0) + e<sub>ùë§</sub>.\n",
    "\n",
    "And we calculate e'<sub>ùë§</sub> for each word in the input sequence, at pos = 1, 2, ‚Ä¶ L - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [PositionalEncoding](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) module below lets us add the positional encoding to the embedding vector. In addition, dropout is also applied to the sums of the embeddings and the positional encodings, in both the Encoder and Decoder stacks. In the original paper, for the base model, a rate of $P_{drop}=0.1$. Note that this is just a hypothetical example using a matrix of zeros, where the embedding dim =20, to showcase the PE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        print(\"dropout:\", dropout)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        print(\"d_model:\", d_model)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(\"pe:\", pe[:,0:2])\n",
    "    \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization from harvardnlp/annotated-transformer GitHub (MIT license)\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "_ = plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the first position's positional encoding values:\n",
    "\n",
    "[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00]\n",
    "\n",
    "This is consistent with our PE calculations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in calculating self-attention is to create three vectors from each of the encoder‚Äôs input vectors (the embeddings). For each word, we create a Query vector, a Key vector, and a Value vector.\n",
    "\n",
    "Now let's see how attention value is calculated. The attention function is defined as below:\n",
    "\n",
    "<img src=\"images/attentionfunction.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how Q, K, V matrices are calculated. The Transformer views the encoded representation of the input as a set of key-value pairs, (K,V), both of dimension d<sub>k</sub>.\n",
    "\n",
    "1. For each word, create a Query vector (q<sub>i</sub>), a Key vector (k<sub>i</sub>), and a Value vector (v<sub>i</sub>). These vectors are created by multiplying the embedding of the word by three matrices that are trained during the training process. \n",
    "2. Calculate the self-attention score for each word. The first word score is q<sub>1</sub>.k<sub>1</sub>, and for the second it is q<sub>1</sub>.k<sub>2</sub>.\n",
    "3. Divide the scores by 8 (the square root of the dimension of the key vectors $\\sqrt{ùëë_k}$ used in the paper).\n",
    "4. Pass the result through a SoftMax operation.\n",
    "5. Multiply each value vector by the SoftMax score: SoftMax x Value.\n",
    "6. Sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 Self-Attention Matrix Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore self-attention matrix calculation in details.\n",
    "\n",
    "The figure below illustrates the matrix calculation of self-attention (compare this with the attention function above). First we pack our embeddings into a matrix <b>X</b>. Each row of the matrix <b>X</b> contains the embedding values of the each word in the input sequence, i.e., </b>X</b> = [<b>x<sub>1</sub></b>, <b>x<sub>2</sub></b>, ..., <b>x<sub>n</sub></b>], where each <b>x<i><sub>i</sub></i></b> is a vector of the embedding values representing the word <i>i</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/attention2.png\" width=\"500\"></center>\n",
    "<center> Figure 5. Self-Attention Matrix Calculation formula. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q</b> matrix is calculated by multiplying <b>X</b> with $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, <b>K</b> matrix is calculated by multiplying <b>X</b> with $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, and <b>V</b> matrix is calculated by multiplying <b>X</b> with $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Visualization of Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook we are exploring the functions that the NVIDIA PyTorch implementation of the Transformer network uses to translate English text into German. The cell below runs the entire network on one setence and prints the output.\n",
    "\n",
    "To run this cell, we must have the following files:\n",
    "* Pre-trained model checkpoints: /data/JoC_Transformer_FP32_PyT_20190304.pt\n",
    "* En-De dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the modules that we need to visualize the self-attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder_demos.self_attention as sa\n",
    "def normalize(arr):\n",
    "    out = np.zeros_like(arr)\n",
    "    for rowno in range(arr.shape[0]):\n",
    "        vals = arr[rowno, :]\n",
    "        out[rowno, :] = (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the weights of one of the self-attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am looking for a place to eat.\"\n",
    "#sentence = \"This is a much more complex sentence and, as a result, is much longer.\"\n",
    "attn, attn_weights = sa.demo(sentence, return_early='self_attn')\n",
    "a_w =attn_weights[0,:, :].cpu().numpy()  #you can print the attention weights.\n",
    "\n",
    "sentence += \" EOS\"\n",
    "sentence = sentence.replace(\".\", \" .\").replace(\",\", \" ,\").split(\" \")\n",
    "\n",
    "#a_w = normalize(a_w)\n",
    "\n",
    "sns.set()\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = sns.heatmap(a_w, vmin=0, vmax=1, cmap=sns.diverging_palette(200,10, n=200), xticklabels=sentence, square=True, yticklabels=sentence, cbar=True,cbar_kws={\"shrink\": .82})\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that the Transformer compares each word (e.g., \"place\") to every other word in the sentence. The result of these comparisons is an attention score/weight for every other word in the sentence. These attention scores determine how much each of the other words should contribute to the next representation of a given word - ‚Äúplace‚Äù for example.\n",
    "\n",
    "The strongest attention links are between some of the last words of the input sentence and the EOS (end of sentence) character. \n",
    "\n",
    "We find it helpful to visualize the strongest attention value for each word. To do this, run the cell above, but uncomment the line that normalizes the attention weights (`a_w = normalize(a_w)`).\n",
    "\n",
    "Now you can see that for the first word \"I\", the most attention is given to the word \"looking\". When it comes to \"eat\", the EOS character takes the highest attention weight. The second highest attention score is given to \"place\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are two heads better than one?  How about eight?  A refinement of self-attention is called ‚Äúmulti-headed‚Äù attention, which allows the model to focus on different positions or sub-spaces. \n",
    "\n",
    "There are h = 8 parallel attention layers, or heads, in the Transformer architecture. This means that there are eight version of self-attention, all running simultaneously.\n",
    "\n",
    "<center><img src=\"images/multiheadattention.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multiheadattention1.png\" width=\"600\"> \n",
    "where $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention is essentially attention repeated several times in parallel. If we do the same self-attention calculation outlined above, h = 8 different times with different weight matrices, we end up with eight different <b>Z</b> matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/multihead.png\"></center>\n",
    "<center> Figure 6. Visual representation of multi-head attention mechanism.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a single attention head has a simple structure: it applies a unique linear transformation to its input queries, keys, and values, computes the attention score between each query and key, then uses it to weight the values and sum them up. The multi-head attention block just applies multiple blocks in parallel, concatenates their outputs, then applies one single linear transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a residual connection is employed around each of the two sub-layers, followed by layer normalization. In mathematical terms, that is, the output of each sub-layer\n",
    "is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \n",
    "\n",
    "Steps to add residual connection, <i><b>X<sup>'</sup></b></i>, to the output of the multi-head attention layer, and then apply layer normalization are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate positional embeddings for the input matrix.\n",
    "\n",
    "<b>X‚Ä≤</b> = PE(<b>X</b>) +  <b>X</b> where <b>X‚Ä≤</b> ‚àà‚Ñù<sup>n<sub>input</sub>√ód<sub>model</sub></sup>.\n",
    "\n",
    "2. Perform multi-head attention layer, and produce the output matrix <b>Z<sup>E</sup><sub>1,1</sub></b>.\n",
    "\n",
    "MultiHead(Q, K, V ) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>)W<sup>O</sup>\n",
    "\n",
    "3. Use residual connection and apply layer normalization to obtain <b>Z<sup>E</sup><sub>1,2</sub></b>  ‚àà‚Ñù<sup>n<sub>input</sub>√ód<sub>model</sub></sup>.\n",
    "\n",
    "<b>Z<sup>E</sup><sub>1,2</sub></b> = LayerNorm(<b>X‚Ä≤</b> + <b>Z<sup>E</sup><sub>1,1</sub></b>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary the left side of the Figure 1 would work like this:\n",
    "     \n",
    "Step1_out = Embedding512 + PositionEncoding512\n",
    "\n",
    "Step2_out = layer_normalization(multihead_attention(Step1_out) + Step1_out)\n",
    "\n",
    "Step3_out = layer_normalization(FFN(Step2_out) + Step2_out)\n",
    "\n",
    "out_enc = Step3_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've learned that \n",
    "* Tranformer encoders have six (default) stacked encoder blocks\n",
    "* An encoder block includes two parts: self-attention and feed forward\n",
    "* Embedding algorithms create vector representations of words\n",
    "* Positional encoding is required because there are no RNNs for sequencing\n",
    "* Multi-head attention allows the model to focus in multiple sub-spaces\n",
    "\n",
    "You'll examine the decoder next - move on to [3.0 Transformer Decoder](030_Decoder.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
